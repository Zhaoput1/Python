# -*- coding: utf-8 -*-
"""sparknlptest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSUW7iuwtCcaanbbUsPWa0Sx-N2rIKhH
"""

import os

# Install java
! apt-get update -qq
! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]
! java -version

# Install pyspark
! pip install --ignore-installed pyspark==2.4.4

# Install Spark NLP
! pip install --ignore-installed spark-nlp==2.5.0

import sparknlp 

spark = sparknlp.start()

print("Spark NLP version: ", sparknlp.version())
print("Apache Spark version: ", spark.version)

from google.colab import drive
drive.mount('/content/drive')

Dataset = spark.read.option("header", True).csv('drive/My Drive/bbc-text.csv')

Dataset.show(10)

df_train, df_test = Dataset.randomSplit([.7, .3])

df_train.show(5)

from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline
import pandas as pd

document_assembler = DocumentAssembler().setInputCol('text').setOutputCol('document')
tokenizer = Tokenizer()\
  .setInputCols(['document'])\
  .setOutputCol('token')
normalizer = Normalizer()\
  .setInputCols(['token'])\
  .setOutputCol('normalized')
stopwords_cleaner = StopWordsCleaner()\
  .setInputCols(['normalized'])\
  .setOutputCol('cleanTokens')\
  .setCaseSensitive(False)
lemma = LemmatizerModel.pretrained('lemma_antbnc')\
  .setInputCols(['cleanTokens'])\
  .setOutputCol('lemma')
word_embeddings = BertEmbeddings\
  .pretrained('bert_base_cased','en')\
  .setInputCols(['document','lemma'])\
  .setOutputCol("embeddings")\
  .setCaseSensitive(False)
embeddingsSentence = SentenceEmbeddings()\
  .setInputCols(['document','embeddings'])\
  .setOutputCol("sentence_embeddings")\
  .setPoolingStrategy('AVERAGE')
classifierdl = ClassifierDLApproach()\
  .setInputCols(['sentence_embeddings'])\
  .setOutputCol('class')\
  .setLabelColumn('category')\
  .setMaxEpochs(5)\
  .setEnableOutputLogs(True)
bert_pipeline = Pipeline(
    stages=[document_assembler,
            tokenizer,
            normalizer,
            stopwords_cleaner,
            lemma,
            word_embeddings,
            embeddingsSentence,
            classifierdl,
    ]
)

bert_pipelineModel = bert_pipeline.fit(df_train)

from sklearn.metrics import classification_report, accuracy_score
df = bert_pipelineModel.transform(df_test).select('category','text','class.result').toPandas()
df['result'] = df['result'].apply(lambda x: x[0])

print(classification_report(df.category, df.result))
print(accuracy_score(df.category, df.result))
